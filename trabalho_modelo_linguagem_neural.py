# -*- coding: utf-8 -*-
"""Trabalho_Modelo_Linguagem_Neural.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gMCJQoSWhT6LnyxPvr4-ZkZluGkc3WcB
"""

!nvidia-smi

!pip install --upgrade transformers accelerate datasets evaluate -q

!git clone https://gitlab.com/checkthat_lab/clef2025-checkthat-lab.git
!ls clef2025-checkthat-lab/task1/data/english

from google.colab import drive
drive.mount('/content/drive')

OUTPUT_DIR = "/content/drive/MyDrive/checkthat_subj_model"

import pandas as pd

train_path = "clef2025-checkthat-lab/task1/data/english/train_en.tsv"

df_head = pd.read_csv(train_path, sep="\t", nrows=5)
df_head

from datasets import load_dataset

data_files = {
    "train": "clef2025-checkthat-lab/task1/data/english/train_en.tsv",
    "validation": "clef2025-checkthat-lab/task1/data/english/dev_en.tsv",
    "dev_test": "clef2025-checkthat-lab/task1/data/english/dev_test_en.tsv",
    "test_labeled": "clef2025-checkthat-lab/task1/data/english/test_en_labeled.tsv",
    "test_unlabeled": "clef2025-checkthat-lab/task1/data/english/test_en_unlabeled.tsv",
}

checkthat = load_dataset(
    "csv",
    data_files=data_files,
    delimiter="\t",
)

#inpeção de uma linha
checkthat["train"][0]

from copy import deepcopy
checkthat2 = deepcopy(checkthat)

label2id = {"OBJ": 0, "SUBJ": 1}
id2label = {0: "OBJ", 1: "SUBJ"}

def encode_labels(example):
    example["labels"] = label2id[example["label"]]
    return example

labeled_splits = ["train", "validation", "dev_test", "test_labeled"]

for split in labeled_splits:
    checkthat2[split] = checkthat2[split].map(encode_labels)

cols_to_keep = ["sentence", "labels", "label"]
for split in labeled_splits:
    cols_to_remove = [c for c in checkthat2[split].column_names if c not in cols_to_keep]
    checkthat2[split] = checkthat2[split].remove_columns(cols_to_remove)

# test_unlabeled mantemos só a sentence
checkthat2["test_unlabeled"] = checkthat2["test_unlabeled"].remove_columns(
    [c for c in checkthat2["test_unlabeled"].column_names if c != "sentence"]
)

from transformers import AutoTokenizer

MODEL_NAME = "roberta-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def tokenize_batch(example):
    return tokenizer(
        example["sentence"],
        truncation=True,
        padding="max_length",
        max_length=256,
    )

checkthat_tok = checkthat2.map(tokenize_batch, batched=True)

for split in labeled_splits:
    checkthat_tok[split].set_format(
        type="torch",
        columns=["input_ids", "attention_mask", "labels"]
    )

import transformers
transformers.__version__

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
import evaluate
import numpy as np

import os
os.environ["WANDB_DISABLED"] = "true"


accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

MODEL_NAME = "roberta-base"

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=2,
    id2label=id2label,
    label2id=label2id,
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels)["accuracy"],
        "f1_macro": f1.compute(predictions=preds, references=labels, average="macro")["f1"],
    }

training_args = TrainingArguments(
    output_dir="/content/checkthat_run",
    # sem evaluation_strategy, sem save_strategy
    learning_rate=2e-5,
    num_train_epochs=30,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    weight_decay=0.01,
    logging_steps=50,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=checkthat_tok["train"],
    eval_dataset=checkthat_tok["validation"],  # ainda podemos usar pra evaluate() depois
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

print("VAL (dev):")
print(trainer.evaluate(checkthat_tok["validation"]))

print("\nDEV-TEST:")
print(trainer.evaluate(checkthat_tok["dev_test"]))

print("\nTEST LABELED:")
print(trainer.evaluate(checkthat_tok["test_labeled"]))

import matplotlib.pyplot as plt

# pega o log do Trainer
logs = trainer.state.log_history

# filtra só entradas que têm 'loss' (treino)
train_steps = [entry["step"] for entry in logs if "loss" in entry]
train_losses = [entry["loss"] for entry in logs if "loss" in entry]

plt.figure(figsize=(8, 5))
plt.plot(train_steps, train_losses)
plt.xlabel("Steps")
plt.ylabel("Training loss")
plt.title("Training loss ao longo dos steps")
plt.grid(True)
plt.show()

import torch
import torch.nn.functional as F

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
model.eval()

def classify_subjectivity(texts, batch_size=32):
    if isinstance(texts, str):
        texts = [texts]

    results = []

    with torch.no_grad():
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]

            enc = tokenizer(
                batch,
                truncation=True,
                padding=True,
                max_length=256,
                return_tensors="pt"
            ).to(device)

            logits = model(**enc).logits
            probs = F.softmax(logits, dim=-1)

            preds = torch.argmax(probs, dim=-1).cpu().tolist()
            probs_subj = probs[:, 1].cpu().tolist()

            for t, p, pr in zip(batch, preds, probs_subj):
                results.append({
                    "text": t,
                    "prediction": id2label[p],      # 'OBJ' ou 'SUBJ'
                    "prob_subj": float(pr)
                })

    return results

from google.colab import drive
drive.mount('/content/drive')

testes = [
    "The government clearly lied and this is outrageous.",
    "GDP rose 3% according to official data."
]

classify_subjectivity(testes)

from google.colab import drive
drive.mount('/content/drive')

SAVE_DIR = "/content/drive/MyDrive/checkthat_subj_model"

trainer.save_model(SAVE_DIR)
tokenizer.save_pretrained(SAVE_DIR)

print("Modelo salvo em:", SAVE_DIR)

"""Segunda Task

"""

!git clone https://github.com/NUSTM/SemEval-2024_ECAC.git
!ls SemEval-2024_ECAC/data

import json

train_path = "SemEval-2024_ECAC/data/Subtask_1_train.json"
test_path  = "SemEval-2024_ECAC/data/Subtask_1_test.json"

with open(train_path, "r") as f:
    sub1_train = json.load(f)

with open(test_path, "r") as f:
    sub1_test = json.load(f)

len(sub1_train), len(sub1_test), sub1_train[0]

def build_context(utterances):
    """
    utterances: lista de dicts com campos tipo:
      - 'speaker'
      - 'text'
      - 'utterance_ID'
    Saída: string com linhas:
      U1_Rachel: bla bla
      U2_Ross: foo bar
    """
    lines = []
    for idx, utt in enumerate(utterances):
        spk = utt.get("speaker", f"Speaker{idx+1}")
        txt = utt["text"]
        lines.append(f"U{utt['utterance_ID']}_{spk}: {txt}")
    return "\n".join(lines)

def conversation_to_qa_examples(conv_item):
    """
    conv_item: um elemento de sub1_train/sub1_test

    Esperado:
      conv_item["conversation_ID"]
      conv_item["conversation"] -> lista de falas
      conv_item["emotion-cause_pairs"] -> lista de pares [ emo_tag, cause_tag ]

    Onde:
      emo_tag   = "3_surprise"
      cause_tag = "1_I realize I am totally naked ."
    """
    conv_id = conv_item["conversation_ID"]
    utterances = conv_item["conversation"]
    pairs = conv_item["emotion-cause_pairs"]

    context = build_context(utterances)
    qa_examples = []

    for emo_tag, cause_tag in pairs:
        # emo_tag = "3_surprise"
        emo_utt_id_str, emotion = emo_tag.split("_", 1)

        # cause_tag = "1_I realize I am totally naked ."
        cause_utt_id_str, cause_span = cause_tag.split("_", 1)

        question = f"What is the cause of the emotion {emotion}?"

        # encontrar o span da causa dentro do context
        start_char = context.find(cause_span)
        if start_char == -1:
            # fallback: se não achar o span exato, pula (pode contar quantos pulou)
            # você pode logar isso depois se quiser debug
            continue

        qa_examples.append({
            "id": f"{conv_id}_{emo_tag}_{cause_utt_id_str}",
            "context": context,
            "question": question,
            "answers": {
                "text": [cause_span],
                "answer_start": [start_char],
            },
            "emotion": emotion,
            "conversation_ID": conv_id,
        })

    return qa_examples

qa_train = []
for item in sub1_train:
    qa_train.extend(conversation_to_qa_examples(item))

qa_test = []
for item in sub1_test:
    qa_test.extend(conversation_to_qa_examples(item))

len(qa_train), len(qa_test), qa_train[0]

from datasets import Dataset, DatasetDict

train_ds = Dataset.from_list(qa_train)
test_ds  = Dataset.from_list(qa_test)

ecac_qa = DatasetDict({
    "train": train_ds,
    "test": test_ds,   # aqui "test" é o dev/test do desafio
})

ecac_qa

from transformers import AutoTokenizer

QA_MODEL_NAME = "roberta-base"
qa_tokenizer = AutoTokenizer.from_pretrained(QA_MODEL_NAME, use_fast=True)

max_length = 384
doc_stride = 128

def prepare_train_features(examples):
    questions = [q.strip() for q in examples["question"]]
    contexts  = examples["context"]
    answers   = examples["answers"]

    tokenized = qa_tokenizer(
        questions,
        contexts,
        truncation="only_second",
        max_length=max_length,
        stride=doc_stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    sample_mapping = tokenized.pop("overflow_to_sample_mapping")
    offset_mapping = tokenized["offset_mapping"]

    start_positions = []
    end_positions = []

    for i, offsets in enumerate(offset_mapping):
        sample_idx = sample_mapping[i]
        answer = answers[sample_idx]
        start_char = answer["answer_start"][0]
        end_char   = start_char + len(answer["text"][0])

        sequence_ids = tokenized.sequence_ids(i)

        # achar início e fim do contexto dentro dos tokens
        token_start_index = 0
        while sequence_ids[token_start_index] != 1:
            token_start_index += 1

        token_end_index = len(offsets) - 1
        while sequence_ids[token_end_index] != 1:
            token_end_index -= 1

        # mover até o primeiro token cujo span se sobrepõe ao início da resposta
        while token_start_index <= token_end_index and offsets[token_start_index][1] <= start_char:
            token_start_index += 1
        # mover até o último token cujo span se sobrepõe ao fim da resposta
        while token_end_index >= token_start_index and offsets[token_end_index][0] >= end_char:
            token_end_index -= 1

        start_positions.append(token_start_index)
        end_positions.append(token_end_index)

    tokenized["start_positions"] = start_positions
    tokenized["end_positions"]   = end_positions

    # não precisamos mais dos offsets no treino
    tokenized.pop("offset_mapping")

    return tokenized

ecac_train_tok = ecac_qa["train"].map(
    prepare_train_features,
    batched=True,
    remove_columns=ecac_qa["train"].column_names,
)

ecac_test_tok = ecac_qa["test"].map(
    prepare_train_features,
    batched=True,
    remove_columns=ecac_qa["test"].column_names,
)

ecac_train_tok[0]

from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer
import torch

qa_model = AutoModelForQuestionAnswering.from_pretrained(QA_MODEL_NAME)

training_args_qa = TrainingArguments(
    output_dir="/content/ecac_subtask1_qa",
    learning_rate=3e-5,
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    logging_steps=50,
    report_to="none",   # nada de W&B
)

qa_trainer = Trainer(
    model=qa_model,
    args=training_args_qa,
    train_dataset=ecac_train_tok,
    eval_dataset=ecac_test_tok,   # vamos usar como dev/test
    tokenizer=qa_tokenizer,
)

qa_trainer.train()

qa_trainer.evaluate()

import torch.nn.functional as F

device = "cuda" if torch.cuda.is_available() else "cpu"
qa_model.to(device)
qa_model.eval()

def predict_cause(context, emotion):
    question = f"What is the cause of the emotion {emotion}?"

    inputs = qa_tokenizer(
        question,
        context,
        return_tensors="pt",
        truncation=True,
        max_length=384,
        padding=True,
    ).to(device)

    with torch.no_grad():
        outputs = qa_model(**inputs)
        start_logits = outputs.start_logits[0]
        end_logits   = outputs.end_logits[0]

    start_idx = torch.argmax(start_logits).item()
    end_idx   = torch.argmax(end_logits).item()
    if end_idx < start_idx:
        end_idx = start_idx

    answer_ids = inputs["input_ids"][0][start_idx:end_idx+1]
    answer_text = qa_tokenizer.decode(answer_ids, skip_special_tokens=True)

    return answer_text

